{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to use ```tf.data.TextLineDataset``` to load examples from text files. ```TextLineDataset``` is designed to create a dataset from text file, in which each example is a line of text from the original file. This is potentially useful for any text data that is primaryly line-based such as poetry or error logs.\n",
    "\n",
    "Thhis example will use 3 different English translations of the same work, Homer's Illiad, and train a model to identify the translator given a single line of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ridzuan\\.keras\\datasets\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/illiad/\"\n",
    "\n",
    "FILE_NAMES = [\n",
    "   \"cowper.txt\", \n",
    "   \"derby.txt\", \n",
    "   \"butler.txt\"\n",
    "]\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "   text_dir = tf.keras.utils.get_file(\n",
    "      name, origin = DIRECTORY_URL + name\n",
    "   )\n",
    "  \n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "print(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text into datasets\n",
    "\n",
    "Iterate through the files, loading each one into its own dataset. Each example needs to be individually labeled, so use ```tf.data.Dataset.map``` to apply a labeler function to each one. This will iterate over every example in the dataset, returning ```(example, label)``` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "   return example, tf.cast(index, tf.int64)\n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "   \n",
    "   lines_dataset = tf.data.TextLineDataset(\n",
    "      os.path.join(parent_dir, file_name)\n",
    "   )\n",
    "   \n",
    "   labeled_dataset = lines_dataset.map(\n",
    "      lambda ex: labeler(ex, i)\n",
    "   )\n",
    "   \n",
    "   labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "\n",
    "# combine these labeled datasets into single dataset and shuffle\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "   all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "   \n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "   BUFFER_SIZE,\n",
    "   reshuffle_each_iteration = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Ajax--Idomeneus--abstain ye both'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Trusting to heav'nly signs, and fav'ring Jove,\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Idomeneus captain of the Cretans was first to make out the running, for'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'finding himself in ambush, but is all the time longing to go into'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"He spake and sat, when Thestor's son arose\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for example in all_labeled_data.take(5):\n",
    "   print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode text line as number\n",
    "\n",
    "Machine learning models work on numbers, not words, so the string values need to be converted into lists of numbers. To do that, map each unique word to a unique integer.\n",
    "\n",
    "#### Vocabulary building\n",
    "\n",
    "Build vocabulary by tokenizing the text into a collection of individual unique words.\n",
    "- Iterate over each example's ```numpy``` value.\n",
    "- Use ```tfds.features.text.Tokenizer``` to split it into tokens.\n",
    "- Collect these token into a set to remove duplicates.\n",
    "- Get size of vocabulary for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  17178\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "   some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "   vocabulary_set.update(some_tokens)\n",
    "   \n",
    "vocab_size = len(vocabulary_set)\n",
    "print(\"Vocabulary size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Create an encoder by passing the ```vocabulary_set``` to ```tfds.features.text.TokenTextEncoder```. The encoder's ```encode``` method takes the string of text and return list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Ajax--Idomeneus--abstain ye both'\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4687, 6673, 9499, 15439, 6050]\n"
     ]
    }
   ],
   "source": [
    "encoded_example_text = encoder.encode(example_text)\n",
    "print(encoded_example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "   encoded_text = encoder.encode(text_tensor.numpy())\n",
    "   return encoded_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```Dataset.map``` to apply the ```encode``` function to each element in the dataset. ```Dataset.map``` runs in graph mode.\n",
    "- Graph tensor do not have a value\n",
    "- In graph mode, only Tensorflow Ops and functions can be used.\n",
    "\n",
    "Therefore ```Dataset.map``` method can't be used directly. Wrap it in a ```tf.py_function```. ```tf.py_function``` will pass regular tensors (with a value and a ```numpy``` method to access it), to the wrapped function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_map_fn(text, label):\n",
    "   # py_func doesn't set the shape of the returned tensor\n",
    "   encoded_text, label = tf.py_function(\n",
    "      encode,\n",
    "      inp = [text, label],\n",
    "      Tout = (tf.int64, tf.int64)\n",
    "   )\n",
    "   \n",
    "   # daaset work best if all components have a shape set\n",
    "   # set the shape manually\n",
    "   \n",
    "   encoded_text.set_shape([None])\n",
    "   label.set_shape([])\n",
    "   \n",
    "   return encoded_text, label\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(\n",
    "   BATCH_SIZE, \n",
    "   padded_shapes = ([None], [])\n",
    ")\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(\n",
    "   BATCH_SIZE, \n",
    "   padded_shapes = ([None], [])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text:  tf.Tensor(\n",
      "[ 4687  6673  9499 15439  6050     0     0     0     0     0     0     0\n",
      "     0     0     0     0], shape=(16,), dtype=int64)\n",
      "Sample label:  tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "print(\"Sample text: \", sample_text[0])\n",
    "print(\"Sample label: \", sample_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since new token encoding (zero used for padding) has been introduced, increase the vocabulary size by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_size + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First layer converts integer representations to dense vecto embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(\n",
    "   tf.keras.layers.Embedding(vocab_size, 64)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long-Short-Term Memory layer lets the model understand words in their context with other words. A bi-directional wrapper on LSTM layer helps it to learn about the datapoints in relationship to the datapoints in relationship to the datapoints that came before it and after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(\n",
    "   tf.keras.layers.Bidirectional(\n",
    "      tf.keras.layers.LSTM(64)\n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Dense layer and output layer. Output layer produces probability for all the labels. The one with the highest probability is the models prediction of an example model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One or more dense layers.\n",
    "# Edit the list in the `for` line to experiment with layer sizes.\n",
    "for units in [64, 64]:\n",
    "   model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "\n",
    "# Output layer. The first argument is the number of labels.\n",
    "model.add(tf.keras.layers.Dense(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          1099456   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,178,115\n",
      "Trainable params: 1,178,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "   optimizer='adam',\n",
    "   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "697/697 [==============================] - 42s 60ms/step - loss: 0.4998 - accuracy: 0.7600 - val_loss: 0.3808 - val_accuracy: 0.8264\n",
      "Epoch 2/3\n",
      "697/697 [==============================] - 35s 51ms/step - loss: 0.2918 - accuracy: 0.8724 - val_loss: 0.3631 - val_accuracy: 0.8300\n",
      "Epoch 3/3\n",
      "697/697 [==============================] - 35s 50ms/step - loss: 0.2171 - accuracy: 0.9057 - val_loss: 0.3828 - val_accuracy: 0.8278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20c2ea29e80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
